{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "80517cdc-b9a4-4c71-9b9d-f129061cd55c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchtext.vocab as vocab\n",
    "import numpy as np\n",
    "from torch import nn\n",
    "from sklearn.preprocessing import LabelEncoder,LabelBinarizer\n",
    "import math\n",
    "import re\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "from wordcloud import WordCloud, STOPWORDS, ImageColorGenerator\n",
    "from nltk.probability import FreqDist\n",
    "import sys\n",
    "from sklearn.model_selection import train_test_split\n",
    "from matplotlib.pylab import plt\n",
    "from numpy import arange\n",
    "from torchsummary import summary\n",
    "import optuna\n",
    "from utilities import cleanLine\n",
    "import pickle\n",
    "import gc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d5217e2-61a0-4e77-9a15-c8d5a4df8c50",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "09560704-e560-4a3b-a8bb-bfa4c05435c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddingName = \"840B\"\n",
    "dimensions = 300\n",
    "eostoken=\"<|eos|>\"\n",
    "sostoken=\"<|sos|>\"\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bfa2bafe-2218-4b92-b68b-1be85ca3575b",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = vocab.GloVe(name=embeddingName,dim=dimensions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d800839d-6d72-496c-be00-95f7e381fbd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8c56e406-ae1b-434f-baa7-4d1b52b0ca1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def getWordEmbedding(word):\n",
    "  return dataset.vectors[dataset.stoi[word]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "fa38082b-f2ba-4915-9364-cb4ab1bf9cb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Loading the preprocessed data\n",
    "file = open(\"preprocessing/preprocessedLines\", 'rb')\n",
    "augmentedLines = pickle.load(file)\n",
    "file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d82b905-512c-42da-af88-6ddb6d02bdc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Define vocabulary, size and create embedding matrix\n",
    "uniq_words = np.unique(np.array(\" \".join(augmentedLines).split(\" \")))\n",
    "uniq_words_idx = np.arange(uniq_words.size)\n",
    "\n",
    "word_to_idx = dict(zip(uniq_words.tolist(), uniq_words_idx.tolist()))\n",
    "idx_to_word = dict(zip(uniq_words_idx.tolist(), uniq_words.tolist()))\n",
    "\n",
    "vocab_size = len(word_to_idx)\n",
    "\n",
    "embedding_matrix = np.zeros((vocab_size, dimensions))\n",
    "\n",
    "unknown_words = []\n",
    "\n",
    "for word, idx in word_to_idx.items():\n",
    "  try: \n",
    "    embedding_matrix[idx] = getWordEmbedding(word)\n",
    "  except KeyError:\n",
    "    if(word==eostoken):\n",
    "        embedding_matrix[idx] = np.zeros((1,dimensions))\n",
    "    elif(word==sostoken):\n",
    "        embedding_matrix[idx] = np.zeros((1,dimensions))\n",
    "    else:\n",
    "        embedding_matrix[idx] = np.random.normal(scale=0.6, size=(dimensions, ))\n",
    "        unknown_words.append(word)\n",
    "\n",
    "print({ 'Number of unknown words': len(unknown_words) })\n",
    "print({ 'Number of known words': vocab_size-len(unknown_words) })\n",
    "print(unknown_words)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "af948df4-b89b-4b87-b82a-6223259f2d69",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create inputs and targets\n",
    "x = []\n",
    "y = []\n",
    "\n",
    "for s in augmentedLines:\n",
    "    x.append(\" \".join(s.split()[:-1]))\n",
    "    y.append(\" \".join(s.split()[1:]))\n",
    "\n",
    "def get_seq_idx(seq):\n",
    "    return [word_to_idx[word] for word in seq.split()]\n",
    "\n",
    "x_idx = np.array([get_seq_idx(word) for word in x])\n",
    "y_idx = np.array([get_seq_idx(word) for word in y])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b648edba-3856-4641-8fbb-0b81dc97be82",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Splitting train, validation and test\n",
    "train_ratio=0.8\n",
    "test_ratio=0.10\n",
    "val_ratio=0.10\n",
    "\n",
    "x_idx_train, x_idx_test, y_idx_train, y_idx_test = train_test_split(x_idx, y_idx, test_size=test_ratio)\n",
    "x_idx_train, x_idx_val, y_idx_train, y_idx_val = train_test_split(x_idx_train, y_idx_train, test_size=val_ratio/(val_ratio + train_ratio))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed8de113-eba2-4397-a8ce-4313b423eb6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset exploring\n",
    "def countWord(words, word):\n",
    "  count=0\n",
    "  for entry in allWords:\n",
    "    if(entry==word):\n",
    "      count+=1\n",
    "  return count\n",
    "\n",
    "def filterStopwords(words, stopwords):\n",
    "  temp = []\n",
    "  for word in words:\n",
    "    if(stopwords.count(word) == 0):\n",
    "      temp.append(word)\n",
    "  return temp\n",
    "\n",
    "def removeEmptyWords(words):\n",
    "  temp = []\n",
    "  for word in words:\n",
    "    if(word!=\"\"):\n",
    "      temp.append(word)\n",
    "  return temp\n",
    "\n",
    "\n",
    "stopwords = []\n",
    "defaultStopwords = list(STOPWORDS)\n",
    "for word in defaultStopwords:\n",
    "  cleanedWord = cleanLine(word, pickle, re)\n",
    "  stopwords.append(cleanedWord)\n",
    "\n",
    "stopwords.append(eostoken)    \n",
    "stopwords.append(sostoken)   \n",
    "allWords = np.array(\" \".join(augmentedLines).split(\" \"))\n",
    "allWords = filterStopwords(allWords, stopwords)\n",
    "allWords = removeEmptyWords(allWords)\n",
    "\n",
    "frequency_distribution = FreqDist(allWords)\n",
    "frequency_distribution.most_common(20)\n",
    "frequency_distribution.plot(20)\n",
    "\n",
    "words = []\n",
    "\n",
    "for word in allWords:\n",
    "  words.append(word)\n",
    "\n",
    "wordtext = \" \".join(words)\n",
    "\n",
    "wordcloud = WordCloud(width=1000, height=500, max_words=200, stopwords=stopwords).generate(wordtext)\n",
    "plt.figure(figsize=(15,10))\n",
    "plt.imshow(wordcloud, interpolation='bilinear')\n",
    "plt.axis(\"off\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "33502224-5ee2-439f-9adc-561e9c114694",
   "metadata": {},
   "outputs": [],
   "source": [
    "class KanyeDataset(torch.utils.data.Dataset):\n",
    "  def __init__(self, x, y, vocab_size):\n",
    "    super(KanyeDataset, self)\n",
    "\n",
    "    self.vocab_size = vocab_size\n",
    "    self.x = torch.from_numpy(x).type(torch.LongTensor).to(device)\n",
    "    self.y = torch.from_numpy(y).type(torch.LongTensor).to(device)\n",
    "\n",
    "  def __len__(self):\n",
    "      return len(self.x)\n",
    "  def __getitem__(self, idx):\n",
    "    x = self.x[idx]\n",
    "    y = self.y[idx]\n",
    "    return x, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d4dd7212-2046-4e56-86f3-e2283fd31901",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTMRNN(nn.Module):\n",
    "  def __init__(self, pretrained_emb, freeze_emb, vocab_size, drop_out, hidden_size, lstm_layers):\n",
    "    super(LSTMRNN, self).__init__()\n",
    "    self.hidden_size = hidden_size\n",
    "    self.num_layers = lstm_layers\n",
    "\n",
    "    self.embed = nn.Embedding.from_pretrained(pretrained_emb, freeze=freeze_emb)\n",
    "    self.lstm = nn.LSTM(dimensions, self.hidden_size, num_layers=self.num_layers, dropout=drop_out, batch_first = True)\n",
    "    self.lin1 = nn.Linear(self.hidden_size, self.hidden_size)\n",
    "    self.lin2 = nn.Linear(self.hidden_size, vocab_size)\n",
    "    self.bn1 = nn.BatchNorm1d(self.hidden_size, device=device)\n",
    "    self.bn2 = nn.BatchNorm1d(vocab_size, device=device)\n",
    "    self.relu = nn.ReLU()\n",
    "    \n",
    "  def forward(self, x, prev_hidden):\n",
    "    out = self.embed(x)\n",
    "    \n",
    "    lstm_output, hidden = self.lstm(out, prev_hidden)\n",
    "    \n",
    "    lin_output = self.lin1(lstm_output)\n",
    "    lin_output = lin_output.transpose(1,2)\n",
    "    lin_output = self.bn1(lin_output)\n",
    "    lin_output = lin_output.transpose(1,2)\n",
    "    lin_output = self.relu(lin_output)\n",
    "    \n",
    "    lin_output = self.lin2(lin_output)\n",
    "    lin_output = lin_output.transpose(1,2)\n",
    "    lin_output = self.bn2(lin_output)\n",
    "    lin_output = lin_output.transpose(1,2)\n",
    "    \n",
    "    return lin_output, hidden\n",
    "  def initHiddenLayer(self, batch_size):\n",
    "    return (torch.zeros(self.num_layers,batch_size,self.hidden_size).to(device),\n",
    "            torch.zeros(self.num_layers,batch_size,self.hidden_size).to(device))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "818348d1-f4a4-47ba-87ac-0e837f7a2ac7",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainSet = KanyeDataset(x_idx_train,y_idx_train, vocab_size)\n",
    "valSet = KanyeDataset(x_idx_val,y_idx_val, vocab_size)\n",
    "testSet = KanyeDataset(x_idx_test,y_idx_test, vocab_size)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d4d7d807-36de-44c7-ae4c-2d6b6d4b4986",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plotTrainingSession(train_loss, val_loss):\n",
    "    num_epochs = len(train_loss)\n",
    "    train_values = train_loss.values()\n",
    "    val_values = val_loss.values()\n",
    " \n",
    "    epochs = range(1, num_epochs+1)\n",
    "\n",
    "    plt.plot(epochs, train_values, label='Training Loss')\n",
    "    plt.plot(epochs, val_values, label='Validation Loss')\n",
    "\n",
    "    plt.title('Training and Validation Loss')\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Loss')\n",
    "\n",
    "    plt.xticks(arange(0, num_epochs+1, 2))\n",
    "\n",
    "    plt.legend(loc='best')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "c2892327-e90b-4122-b3ea-748f8715ada0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "def trainModel(model, dataloader, valloader, num_epochs, batch_size, loss_fn, optim, lr_scheduler, printresults = False, printsessionreport = False):\n",
    "    best_val_loss = np.inf\n",
    "    best_avg_val_loss = np.inf\n",
    "    train_loss = {}\n",
    "    val_loss = {}\n",
    "    strike = 0\n",
    " \n",
    "    for epoch in range(num_epochs):\n",
    "        # Initialize the hidden layer\n",
    "        hidden_layer = model.initHiddenLayer(batch_size)\n",
    "        h = hidden_layer[0]\n",
    "        c = hidden_layer[1]\n",
    "        epoch_train_loss = 0\n",
    "        \n",
    "        model.train()\n",
    "        for xbatch, ybatch in dataloader:\n",
    "            model.zero_grad()\n",
    "            output, (h, c) = model(xbatch, (h,c))\n",
    "\n",
    "            h = h.detach()\n",
    "            c = c.detach()\n",
    "            \n",
    "            # Compute loss\n",
    "            loss = loss_fn(output.transpose(1,2), ybatch)\n",
    "            \n",
    "            epoch_train_loss += loss.item()\n",
    "            \n",
    "            # Do backward propagation\n",
    "            loss.backward()\n",
    "            optim.step()\n",
    "        \n",
    "        epoch_val_loss = 0\n",
    "        model.eval()\n",
    "        \n",
    "        # Run validation epoch\n",
    "        for xbatch, ybatch in valloader:\n",
    "            output, (h, c) = model(xbatch, (h,c))\n",
    "\n",
    "            h = h.detach()\n",
    "            c = c.detach()\n",
    "\n",
    "            loss = loss_fn(output.transpose(1,2), ybatch)\n",
    "            epoch_val_loss += loss.item()\n",
    "            \n",
    "        #If result is the best so far, save it - else terminate    \n",
    "        if(epoch_val_loss<best_val_loss):\n",
    "            best_val_loss = epoch_val_loss\n",
    "            best_avg_val_loss = epoch_val_loss/len(valloader)\n",
    "            bestModel=copy.deepcopy(model.state_dict())\n",
    "        else:\n",
    "            strike = 1\n",
    "            \n",
    "        if(strike==1):\n",
    "            print(\"Early stopping due to lack of validation improvement\")\n",
    "            break\n",
    "        \n",
    "        average_train_loss = epoch_train_loss/len(dataloader)\n",
    "        average_val_loss = epoch_val_loss/len(valloader)\n",
    "        \n",
    "        train_loss[epoch] = average_train_loss\n",
    "        val_loss[epoch] = average_val_loss\n",
    "        if(printresults):\n",
    "            print({ 'Number epoch': epoch+1, 'Average train loss': average_train_loss, 'Average val loss': average_val_loss })\n",
    "        \n",
    "        # Run step on the LR scheduler\n",
    "        lr_scheduler.step()\n",
    "    \n",
    "    if(printsessionreport):\n",
    "        plotTrainingSession(train_loss, val_loss)\n",
    "    return bestModel, best_avg_val_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "36ee16e1-7195-4e2b-a4ac-2ea6ec220edf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Returns a model based on hyper parameters\n",
    "def buildModel(params):\n",
    "    drop_out_lstm = params['drop_out_lstm']\n",
    "    lstm_layers = params['lstm_layers']\n",
    "    hidden_size = params['hidden_size']\n",
    "    freeze_emb = params['freeze_emb']\n",
    "    if freeze_emb == 1:\n",
    "        freeze_emb = True\n",
    "    else:\n",
    "        freeze_emb = False\n",
    "    \n",
    "    model = LSTMRNN(torch.from_numpy(embedding_matrix).float(), freeze_emb, vocab_size, drop_out_lstm, hidden_size, lstm_layers)\n",
    "    model.to(device)\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "5be236cb-d718-411a-b7b6-31bbee617d75",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Objective function for Optuna to minimize\n",
    "def objective(trial):    \n",
    "\n",
    "    params = {\n",
    "          'drop_out_lstm': trial.suggest_float(\"drop_out_lstm\", 0.2, 0.6, step=0.2),\n",
    "          'lstm_layers': trial.suggest_int(\"lstm_layers\", 2, 4, step=1),\n",
    "          'hidden_size': trial.suggest_int(\"hidden_size\", 128, 512, step=128),\n",
    "        'freeze_emb': trial.suggest_int(\"freeze_emb\", 0, 1, step=1),\n",
    "        'batch_size': trial.suggest_int(\"batch_size\", 16, 64, step=16),\n",
    "        'decay_rate': trial.suggest_float(\"decay_rate\", 0.80, 0.96, step=0.2),\n",
    "        'lr': trial.suggest_float(\"lr\", 0.001, 0.004, step=0.0005),\n",
    "        'weight_decay': trial.suggest_float(\"weight_decay\", 0.0, 0.1, step=0.5),\n",
    "    }\n",
    "\n",
    "    model = buildModel(params)\n",
    "\n",
    "    num_epochs = 30\n",
    "    batch_size = params['batch_size']\n",
    "    decayRate = params['decay_rate']\n",
    "    lr = params['lr']\n",
    "    weight_decay = params['weight_decay']\n",
    "    \n",
    "\n",
    "    trainloader = torch.utils.data.DataLoader(trainSet, \n",
    "                                          batch_size=batch_size, \n",
    "                                          shuffle=True,\n",
    "                                          drop_last=True)\n",
    "    valloader = torch.utils.data.DataLoader(valSet,\n",
    "                                            batch_size=batch_size,\n",
    "                                            shuffle=True,\n",
    "                                            drop_last=True)\n",
    "\n",
    "    optimizer = torch.optim.AdamW(model.parameters(), lr = lr, weight_decay=weight_decay)\n",
    "    lr_sched = torch.optim.lr_scheduler.ExponentialLR(optimizer=optimizer, gamma=decayRate)\n",
    "    loss_func = nn.CrossEntropyLoss()\n",
    "    \n",
    "    bestModel, val_loss = trainModel(model,\n",
    "                                     trainloader,\n",
    "                                     valloader,\n",
    "                                     num_epochs=num_epochs,\n",
    "                                     batch_size=batch_size,\n",
    "                                     loss_fn=loss_func, \n",
    "                                     optim=optimizer,\n",
    "                                     lr_scheduler = lr_sched,\n",
    "                                     printresults=True,\n",
    "                                     printsessionreport=True)\n",
    "\n",
    "    return val_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e2026db-e710-4d1f-acf5-e85a346a8396",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run hypertuning and save results\n",
    "study = optuna.create_study(direction=\"minimize\", sampler=optuna.samplers.TPESampler())\n",
    "study.optimize(objective, n_trials=100)\n",
    "\n",
    "studyname = \"Study\"\n",
    "file = open(studyname, 'wb')\n",
    "pickle.dump(study, file)\n",
    "file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af6a078c-c5f3-4ea4-a434-cf164bc7cb5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train with best parameters\n",
    "params = {\n",
    "      'drop_out_lstm': 0.2,\n",
    "      'lstm_layers': 2,\n",
    "      'hidden_size': 512,\n",
    "    'freeze_emb': 0,\n",
    "    'num_epochs': 40,\n",
    "    'batch_size': 20,\n",
    "    'decay_rate': 0.86,\n",
    "    'lr': 0.0035,\n",
    "    'weight_decay': 0.05,\n",
    "}\n",
    "\n",
    "model = buildModel(params)\n",
    "\n",
    "num_epochs = params['num_epochs']\n",
    "batch_size = params['batch_size']\n",
    "decayRate = params['decay_rate']\n",
    "lr = params['lr']\n",
    "weight_decay = params['weight_decay']\n",
    "\n",
    "trainloader = torch.utils.data.DataLoader(trainSet, \n",
    "                                      batch_size=batch_size, \n",
    "                                      shuffle=True,\n",
    "                                      drop_last=True)\n",
    "valloader = torch.utils.data.DataLoader(valSet,\n",
    "                                        batch_size=batch_size,\n",
    "                                        shuffle=True,\n",
    "                                        drop_last=True)\n",
    "\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr = lr, weight_decay=weight_decay)\n",
    "lr_sched = torch.optim.lr_scheduler.ExponentialLR(optimizer=optimizer, gamma=decayRate)\n",
    "loss_func = nn.CrossEntropyLoss()\n",
    "\n",
    "bestModel, val_loss = trainModel(model,\n",
    "                                 trainloader,\n",
    "                                 valloader,\n",
    "                                 num_epochs=num_epochs,\n",
    "                                 batch_size=batch_size,\n",
    "                                 loss_fn=loss_func, \n",
    "                                 optim=optimizer,\n",
    "                                 lr_scheduler = lr_sched,\n",
    "                                 printresults=True,\n",
    "                                 printsessionreport=True)\n",
    "\n",
    "# Save model parameters and the testset for it.\n",
    "file = open(\"testset\", 'wb')\n",
    "pickle.dump(testSet, file)\n",
    "file.close()\n",
    "\n",
    "file = open(\"model\", 'wb')\n",
    "pickle.dump(bestModel, file)\n",
    "file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "f24e3fe8-c471-4d2b-b188-928743cf0e51",
   "metadata": {},
   "outputs": [],
   "source": [
    "file = open(\"model\", 'rb')\n",
    "bestModel = pickle.load(file)\n",
    "file.close()\n",
    "\n",
    "file = open(\"testset\", 'rb')\n",
    "testSet = pickle.load(file)\n",
    "file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1cf734a-48f8-4985-a0ee-9150e9c3694d",
   "metadata": {},
   "outputs": [],
   "source": [
    "testloader = torch.utils.data.DataLoader(testSet,\n",
    "                                        batch_size=1,\n",
    "                                        shuffle=True,\n",
    "                                        drop_last=True)\n",
    "\n",
    "model = LSTMRNN(torch.from_numpy(embedding_matrix).float(), 0, vocab_size, 0.2, 512, 2)\n",
    "model.to(device)\n",
    "model.load_state_dict(bestModel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "9a14eaa1-a31c-4376-a92c-a9095fc82edb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "# Generate one line\n",
    "def makeline(model,text,linelength):\n",
    "    model.eval()\n",
    "    h,c = model.initHiddenLayer(1)\n",
    "    for i in range(len(text),linelength):\n",
    "        x = torch.tensor([[word_to_idx[w] for w in text]]).to(device)\n",
    "        y_pred, (h, c) = model(x, (h, c))\n",
    "        last_word_logits = y_pred[0][-1]\n",
    "        p = torch.nn.functional.softmax(last_word_logits, dim=0).detach().cpu().numpy()\n",
    "        top_n_idx = p.argmax()\n",
    "        #top_n_idx = p.argsort()[-3:][::-1]\n",
    "        #sampled_token_index = top_n_idx[random.sample([0,1,2],1)[0]]\n",
    "        nextword = idx_to_word[top_n_idx]\n",
    "        \n",
    "        if(nextword==eostoken):\n",
    "            text.append(eostoken)\n",
    "            break\n",
    "        else:\n",
    "            text.append(nextword)\n",
    "    \n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37791dd2-5faa-40d1-9ecf-c4b90bfc8d72",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Wrapper to format input output\n",
    "def makelyric(input, model, sentencemaxlength):\n",
    "    input = sostoken+ \" \"+input\n",
    "    alterinput = input.split()\n",
    "    output = makeline(model,alterinput,sentencemaxlength)\n",
    "    output = \" \".join(output)\n",
    "    print({'Prompt    ': input})\n",
    "    print({'Generated:': output})\n",
    "    print(\"\\n\")\n",
    "\n",
    "makelyric(\"all i want is\", model, 20)\n",
    "makelyric(\"can i get a\", model, 20)\n",
    "makelyric(\"and i know what they\", model, 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "e644f52b-e8f2-43a1-a842-42a10332fb8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.translate.bleu_score import corpus_bleu\n",
    "\n",
    "#Calculate BLEU on generated and reference sentences\n",
    "def bleu(generated, target):\n",
    "    ref_bleu = []\n",
    "    gen_bleu = []\n",
    "    for l in generated:\n",
    "        gen_bleu.append(l.split())\n",
    "    for i,l in enumerate(target):\n",
    "        ref_bleu.append([l.split()])\n",
    "\n",
    "    score_bleu = corpus_bleu(ref_bleu, gen_bleu)\n",
    "    return score_bleu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "18611778-4f41-4540-896d-18a78da184ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run evaluation\n",
    "def evaluate(testloader, model, promptlength):\n",
    "    model.eval()\n",
    "    refs = []\n",
    "    gens = []\n",
    "    for xbatch, ybatch in testloader:\n",
    "        x = [idx_to_word[token] for token in xbatch.squeeze().cpu().numpy()]\n",
    "        y = [idx_to_word[token] for token in ybatch.squeeze().cpu().numpy()]\n",
    "\n",
    "        refsentence = x\n",
    "        refsentence = np.append(refsentence, y[-1])\n",
    "        \n",
    "        words = xbatch.squeeze()[:promptlength].cpu().numpy()\n",
    "        words = [idx_to_word[token] for token in words]\n",
    "        \n",
    "        gensentence = makeline(model, words, 16)  \n",
    "        \n",
    "        gensentence = gensentence[promptlength:]\n",
    "        refsentence = refsentence[promptlength:]\n",
    "\n",
    "        for i, w in enumerate(refsentence):\n",
    "            if(w==eostoken):\n",
    "                index = i\n",
    "                refsentence = refsentence[:index+1]\n",
    "                break\n",
    "                \n",
    "        ref = \" \".join(refsentence)\n",
    "        gen = \" \".join(gensentence)\n",
    "        \n",
    "        refs.append(ref)\n",
    "        gens.append(gen)\n",
    "    return gens, refs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "4bf8b9cf-f5d2-4743-b2b8-35deb44344da",
   "metadata": {},
   "outputs": [],
   "source": [
    "p2 = evaluate(testloader, model, 2)\n",
    "p3 = evaluate(testloader, model, 3)\n",
    "p4 = evaluate(testloader, model, 4)\n",
    "p5 = evaluate(testloader, model, 5)\n",
    "p6 = evaluate(testloader, model, 6)\n",
    "p7 = evaluate(testloader, model, 7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "618a4a8a-3191-45a9-8aed-2db776db2809",
   "metadata": {},
   "outputs": [],
   "source": [
    "p = [p2,p3,p4,p5,p6,p7]\n",
    "file = open(\"evaluations\", 'wb')\n",
    "pickle.dump(p, file)\n",
    "file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "22fbe75a-fa2b-43e3-a100-c9bd6436b1b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "file = open(\"evaluations\", 'rb')\n",
    "p = pickle.load(file)\n",
    "file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "71b0fd32-bb99-4de2-bbbe-f7de1bea2a4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Calculate BLEU\n",
    "b2 = bleu(p[0][0], p[0][1])\n",
    "b3 = bleu(p[1][0], p[1][1])\n",
    "b4 = bleu(p[2][0], p[2][1])\n",
    "b5 = bleu(p[3][0], p[3][1])\n",
    "b6 = bleu(p[4][0], p[4][1])\n",
    "b7 = bleu(p[5][0], p[5][1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "47cb931b-0347-48e4-8eb5-e492d2609915",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plotScores(bleuscores, roguescores, promptlengths):\n",
    "    assert(len(bleuscores)==len(promptlengths))\n",
    "    assert(len(roguescores)==len(bleuscores))\n",
    "    plt.plot(promptlengths, bleuscores, label='BLEU')\n",
    "    plt.plot(promptlengths, roguescores, label='ROGUE')\n",
    "\n",
    "    plt.title('Scores')\n",
    "    plt.xlabel('Prompt length')\n",
    "    plt.ylabel('Score')\n",
    "\n",
    "    plt.xticks(arange(promptlengths[0], promptlengths[-1]+1, 1))\n",
    "    plt.yticks(arange(0, 1.1, 0.1))\n",
    "\n",
    "    plt.legend(loc='best')\n",
    "    plt.show()\n",
    "    \n",
    "    return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "5c044270-ed62-457a-9c88-210d8b2034c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Taken directly from https://towardsdatascience.com/how-to-evaluate-text-generation-models-metrics-for-automatic-evaluation-of-nlp-models-e1c251b04ec1\n",
    "\n",
    "import itertools\n",
    "def _split_into_words(sentences):\n",
    "  \"\"\"Splits multiple sentences into words and flattens the result\"\"\"\n",
    "  return list(itertools.chain(*[_.split(\" \") for _ in sentences]))\n",
    "\n",
    "#supporting function\n",
    "def _get_word_ngrams(n, sentences):\n",
    "  \"\"\"Calculates word n-grams for multiple sentences.\n",
    "  \"\"\"\n",
    "  assert len(sentences) > 0\n",
    "  assert n > 0\n",
    "\n",
    "  words = _split_into_words(sentences)\n",
    "  return _get_ngrams(n, words)\n",
    "\n",
    "#supporting function\n",
    "def _get_ngrams(n, text):\n",
    "  \"\"\"Calcualtes n-grams.\n",
    "  Args:\n",
    "    n: which n-grams to calculate\n",
    "    text: An array of tokens\n",
    "  Returns:\n",
    "    A set of n-grams\n",
    "  \"\"\"\n",
    "  ngram_set = set()\n",
    "  text_length = len(text)\n",
    "  max_index_ngram_start = text_length - n\n",
    "  for i in range(max_index_ngram_start + 1):\n",
    "    ngram_set.add(tuple(text[i:i + n]))\n",
    "  return ngram_set\n",
    "\n",
    "def rouge_n(reference_sentences, evaluated_sentences, n=2):\n",
    "  \"\"\"\n",
    "  Computes ROUGE-N of two text collections of sentences.\n",
    "  Source: http://research.microsoft.com/en-us/um/people/cyl/download/\n",
    "  papers/rouge-working-note-v1.3.1.pdf\n",
    "  Args:\n",
    "    evaluated_sentences: The sentences that have been picked by the summarizer\n",
    "    reference_sentences: The sentences from the referene set\n",
    "    n: Size of ngram.  Defaults to 2.\n",
    "  Returns:\n",
    "    recall rouge score(float)\n",
    "  Raises:\n",
    "    ValueError: raises exception if a param has len <= 0\n",
    "  \"\"\"\n",
    "  if len(evaluated_sentences) <= 0 or len(reference_sentences) <= 0:\n",
    "    raise ValueError(\"Collections must contain at least 1 sentence.\")\n",
    "\n",
    "  evaluated_ngrams = _get_word_ngrams(n, evaluated_sentences)\n",
    "  reference_ngrams = _get_word_ngrams(n, reference_sentences)\n",
    "  reference_count = len(reference_ngrams)\n",
    "  evaluated_count = len(evaluated_ngrams)\n",
    "\n",
    "  overlapping_ngrams = evaluated_ngrams.intersection(reference_ngrams)\n",
    "  overlapping_count = len(overlapping_ngrams)\n",
    "\n",
    "  if evaluated_count == 0:\n",
    "    precision = 0.0\n",
    "  else:\n",
    "    precision = overlapping_count / evaluated_count\n",
    "\n",
    "  if reference_count == 0:\n",
    "    recall = 0.0\n",
    "  else:\n",
    "    recall = overlapping_count / reference_count\n",
    "\n",
    "  f1_score = 2.0 * ((precision * recall) / (precision + recall + 1e-8))\n",
    "\n",
    "  return recall\n",
    "\n",
    "# Compute scores\n",
    "r2 = rouge_n(p[0][1], p[0][0])\n",
    "r3 = rouge_n(p[1][1], p[1][0])\n",
    "r4 = rouge_n(p[2][1], p[2][0])\n",
    "r5 = rouge_n(p[3][1], p[3][0])\n",
    "r6 = rouge_n(p[4][1], p[4][0])\n",
    "r7 = rouge_n(p[5][1], p[5][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbcbfaa6-92b7-422c-aca1-d41c02a3113d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot both scores\n",
    "plotScores([b2,b3,b4,b5,b6,b7], [r2,r3,r4,r5,r6,r7], [2,3,4,5,6,7])\n",
    "[b2,b3,b4,b5,b6,b7]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
