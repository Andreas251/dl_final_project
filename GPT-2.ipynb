{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inspiration taken from\n",
    "# https://towardsdatascience.com/how-to-fine-tune-gpt-2-for-text-generation-ae2ea53bc272\n",
    "# https://towardsdatascience.com/foundations-of-nlp-explained-bleu-score-and-wer-metrics-1a5ba06d812b\n",
    "# https://towardsdatascience.com/hyperparameter-tuning-of-neural-networks-with-optuna-and-pytorch-22e179efc837"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from transformers import GPT2LMHeadModel, GPT2Tokenizer\n",
    "import numpy as np\n",
    "import random\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import GPT2Tokenizer, GPT2LMHeadModel, AdamW, get_linear_schedule_with_warmup\n",
    "from tqdm import tqdm, trange\n",
    "import torch.nn.functional as F\n",
    "import csv\n",
    "import os\n",
    "import pickle\n",
    "from sklearn.model_selection import train_test_split\n",
    "import optuna\n",
    "import matplotlib.pyplot as plt\n",
    "from numpy import arange\n",
    "from nltk.translate.bleu_score import corpus_bleu, SmoothingFunction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_bleu_scores(gpt2_scores, rnn_scores, prompt_lengths):\n",
    "\n",
    "    plt.plot(prompt_lengths, gpt2, label='GPT-2')\n",
    "    plt.plot(prompt_lengths, rnn, label='RNN')\n",
    "\n",
    "    plt.title('BLEU scores')\n",
    "    plt.xlabel('Prompt length')\n",
    "    plt.ylabel('Score')\n",
    "\n",
    "    plt.xticks(arange(2, 7, 1))\n",
    "\n",
    "    plt.legend(loc='best')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_large_dataset():\n",
    "    file = open(\"preprocessing/preprocessedLines\", 'rb')\n",
    "    augmented_lines = pickle.load(file)\n",
    "    stripped_lines = []\n",
    "    file.close()\n",
    "    \n",
    "    # Strip sos and eos\n",
    "    for line in augmented_lines:\n",
    "        new_line = line.replace(\"<|eos|>\", \"\").replace(\"<|sos|>\", \"\").replace(\"<|pad|>\", \"\")\n",
    "        stripped_lines.append(new_line)\n",
    "        \n",
    "    df = pd.DataFrame(stripped_lines, columns=[\"Lyrics\"])\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_small_dataset():\n",
    "    txt_file = \"data/kanye_verses.txt\"\n",
    "\n",
    "    df = pd.read_fwf(txt_file)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_dataset(dataset, train_ratio = 0.8, val_ratio = 0.1, test_ratio=0.1):\n",
    "    val_size = val_ratio/(val_ratio+train_ratio)\n",
    "    \n",
    "    X_train, X_test = train_test_split(dataset, test_size=test_ratio, shuffle=False)\n",
    "    X_train, X_val = train_test_split(X_train, test_size=val_size, shuffle=False)\n",
    "\n",
    "    return X_train, X_val, X_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenize lyrics\n",
    "class SongLyrics(Dataset):  \n",
    "    def __init__(self, kanye_lyrics, truncate=False, gpt2_type=\"gpt2\", max_length=1024):\n",
    "\n",
    "        self.tokenizer = GPT2Tokenizer.from_pretrained(gpt2_type) # Get GPT2 tokenizer\n",
    "        self.lyrics = []\n",
    "\n",
    "        for row in kanye_lyrics[\"Lyrics\"]:\n",
    "        \n",
    "            sample = torch.tensor(self.tokenizer.encode(f\"{row[:max_length]}<|endoftext|>\"))\n",
    "            paddingLength = max_length-len(sample)\n",
    "            for i in range(paddingLength):\n",
    "                sample = torch.cat([sample, torch.tensor(self.tokenizer.encode(f\"<|endoftext|>\"))])\n",
    "                \n",
    "            if sample.size()[0] <= max_length:\n",
    "                self.lyrics.append(sample) # Encode each line up to max length\n",
    "        \n",
    "        if truncate:\n",
    "            self.lyrics = self.lyrics[:20000]\n",
    "        self.lyrics_count = len(self.lyrics)\n",
    "        \n",
    "    def __len__(self):\n",
    "        return self.lyrics_count\n",
    "\n",
    "    def __getitem__(self, item):\n",
    "        return self.lyrics[item]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Getting data, splitting it and getting tokenizer and model.\n",
    "#kanye_lyrics = get_small_dataset()\n",
    "kanye_lyrics = get_large_dataset()\n",
    "\n",
    "dataset = SongLyrics(kanye_lyrics, truncate=False, gpt2_type=\"gpt2\", max_length=50)\n",
    "\n",
    "X_train, X_val, X_test = split_dataset(dataset)\n",
    "\n",
    "tokenizer = GPT2Tokenizer.from_pretrained('gpt2')\n",
    "model = GPT2LMHeadModel.from_pretrained('gpt2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_training_session(train_loss, val_loss):\n",
    "    num_epochs = len(train_loss)\n",
    "    train_values = train_loss.values()\n",
    "    val_values = val_loss.values()\n",
    " \n",
    "    epochs = range(1, num_epochs+1)\n",
    "\n",
    "    plt.plot(epochs, train_values, label='Training Loss')\n",
    "    plt.plot(epochs, val_values, label='Validation Loss')\n",
    "\n",
    "    plt.title('Training and Validation Loss')\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Loss')\n",
    "\n",
    "    plt.xticks(arange(0, num_epochs+1, 2))\n",
    "\n",
    "    plt.legend(loc='best')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def finetune(\n",
    "    train_dataloader, val_dataloader, model, tokenizer,\n",
    "    epochs=5, lr=2e-5, weight_decay=0.05,\n",
    "    max_seq_len=200, warmup_steps=50, print_report=True\n",
    "):\n",
    "    device=torch.device(\"cuda\")\n",
    "    model = model.cuda()\n",
    "    model.train()\n",
    "\n",
    "    optimizer = torch.optim.AdamW(model.parameters(), lr=lr, weight_decay=weight_decay) # weight decay default is 0.01\n",
    "    \n",
    "    scheduler = get_linear_schedule_with_warmup(\n",
    "        optimizer, num_warmup_steps=warmup_steps, num_training_steps=-1\n",
    "    )\n",
    "\n",
    "    best_val_loss = np.inf\n",
    "    train_loss_list = {}\n",
    "    val_loss_list = {}\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        train_loss = 0\n",
    "        val_loss = 0\n",
    "\n",
    "        for entry in tqdm(train_dataloader): # progress bar\n",
    "            entry = entry.to(device)\n",
    "            outputs = model(entry, labels=entry) # labels by shifting\n",
    "            \n",
    "            loss = outputs[0]\n",
    "            train_loss += loss.item()\n",
    "            \n",
    "            loss.backward()\n",
    "\n",
    "            optimizer.step()\n",
    "            scheduler.step()\n",
    "            model.zero_grad()\n",
    "            \n",
    "        # Val loss\n",
    "        for entry in val_dataloader:\n",
    "            entry = entry.to(device)\n",
    "            outputs = model(entry, labels=entry)\n",
    "            \n",
    "            loss = outputs[0]\n",
    "            val_loss += loss.item()\n",
    "        \n",
    "        val_loss_avg = val_loss/len(val_dataloader)\n",
    "        train_loss_avg = train_loss/len(train_dataloader)\n",
    "        \n",
    "        train_loss_list[epoch] = train_loss_avg\n",
    "        val_loss_list[epoch] = val_loss_avg\n",
    "        \n",
    "        if print_report:\n",
    "            print(\"--------------------------------------------\")\n",
    "            print(\"Epoch \" + str(epoch + 1))\n",
    "            print(\"Avg. train loss: \" + str(train_loss_avg))\n",
    "            print(\"Avg. validation loss: \" + str(val_loss_avg))\n",
    "            print(\"--------------------------------------------\")\n",
    "          \n",
    "        if val_loss_avg > best_val_loss:\n",
    "            print(\"Stopping due to worse validation loss\")\n",
    "            print(\"Best validation loss: \" + str(best_val_loss))\n",
    "            break\n",
    "        else:\n",
    "            best_val_loss = val_loss_avg\n",
    "            \n",
    "    if print_report:\n",
    "        plot_training_session(train_loss_list, val_loss_list)\n",
    "    \n",
    "    return model, best_val_loss # Returning best val loss for the HPO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate outputs\n",
    "def generate(\n",
    "    model,\n",
    "    tokenizer,\n",
    "    prompt,\n",
    "    entry_length=200\n",
    "):\n",
    "    model.eval()\n",
    "    vocab = list(tokenizer.encoder.values())\n",
    "\n",
    "    with torch.no_grad():\n",
    "        entry_finished = False\n",
    "        generated = torch.tensor(tokenizer.encode(prompt))\n",
    "\n",
    "        for i in range(entry_length): \n",
    "            outputs = model(generated, labels=generated)\n",
    "            loss, logits = outputs[:2]\n",
    "            logits = logits[-1, :]\n",
    "            p = F.softmax(logits, dim=-1).detach().cpu().numpy()\n",
    "            \n",
    "            #top_n_idx = p.argsort()[-3:][::-1]\n",
    "            #sampled_token_index = top_n_idx[random.sample([0,1,2],1)[0]] # If we want randomized generation\n",
    "            sampled_token_index = p.argmax()\n",
    "            \n",
    "            next_token = torch.tensor(vocab[sampled_token_index]).unsqueeze(0)\n",
    "            \n",
    "            generated = torch.cat((generated, next_token), dim=0)\n",
    "\n",
    "            if next_token in tokenizer.encode(\"<|endoftext|>\"):\n",
    "                break\n",
    "        \n",
    "        output_list = list(generated.squeeze().numpy())\n",
    "        output_text = tokenizer.decode(output_list)\n",
    "        \n",
    "        return output_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Objective for Optuna hyperparameter optimization\n",
    "def objective(trial):\n",
    "    params = {\n",
    "        \"learning_rate\": trial.suggest_float(\"learning_rate\", 1e-5, 1e-1),\n",
    "        \"weight_decay\": trial.suggest_float(\"weight_decay\", 0.05, 0.20),\n",
    "        \"epochs\": trial.suggest_int(\"epochs\", 5, 20),\n",
    "        \"batch_size\": trial.suggest_int(\"batch_size\", 4, 32)\n",
    "        }\n",
    "    \n",
    "    train_dataloader = DataLoader(X_train, batch_size=params[\"batch_size\"], shuffle=True, drop_last=True)\n",
    "    val_dataloader = DataLoader(X_val, batch_size=params[\"batch_size\"], shuffle=True, drop_last=True)\n",
    "    \n",
    "    model = GPT2LMHeadModel.from_pretrained('gpt2')\n",
    "    \n",
    "    model, val_loss = finetune(train_dataloader, val_dataloader, model, tokenizer, \n",
    "                               epochs=params[\"epochs\"], \n",
    "                               lr=params[\"learning_rate\"], \n",
    "                               weight_decay=params[\"weight_decay\"],\n",
    "                               print_report=False\n",
    "                              )\n",
    "    \n",
    "    return val_loss\n",
    "\n",
    "\n",
    "#study = optuna.create_study(direction=\"minimize\", sampler=optuna.samplers.TPESampler()) # Bayesian sampler\n",
    "#study.optimize(objective, n_trials=10)\n",
    "\n",
    "#file = open(\"large_model\", 'wb')\n",
    "#pickle.dump(study, file)\n",
    "#file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculating perplexity, which we decided not to include in report\n",
    "def perplexity(model, dataloader):\n",
    "    device=torch.device(\"cuda\")\n",
    "    ppl_acc = 0\n",
    "    \n",
    "    for entry in dataloader:\n",
    "        entry = entry.to(device)\n",
    "        outputs = model(entry, labels=entry)\n",
    "\n",
    "        loss = outputs[0]\n",
    "        \n",
    "        ppl = torch.exp(loss)\n",
    "        ppl_acc += ppl\n",
    "    \n",
    "    average_test_ppl = ppl_acc/len(dataloader)\n",
    "\n",
    "    print({ 'Average ppl': average_test_ppl.item() })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#filename = \"small_model\"\n",
    "#epochs = 15\n",
    "#batch_size = 26\n",
    "#weight_decay = 0.1832593146816496\n",
    "#learning_rate =  0.001015043804472181\n",
    "\n",
    "filename = \"large_model\"\n",
    "epochs = 6\n",
    "batch_size = 31\n",
    "weight_decay = 0.14375047857667853\n",
    "learning_rate =  0.008253568893666966\n",
    "\n",
    "train_dataloader = DataLoader(X_train, batch_size=batch_size, shuffle=True, drop_last=True)\n",
    "val_dataloader = DataLoader(X_val, batch_size=batch_size, shuffle=True, drop_last=True)\n",
    "test_dataloader = DataLoader(X_test, batch_size=batch_size, shuffle=True, drop_last=True)\n",
    "\n",
    "model, val_loss = finetune(train_dataloader, val_dataloader, model, tokenizer, epochs=epochs, weight_decay=weight_decay, lr=learning_rate) # Set batch_size\n",
    "\n",
    "# Save model\n",
    "#file = open(\"filename\", 'wb')\n",
    "#pickle.dump(model.state_dict(), file)\n",
    "#file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bleu_score(model, dataloader):\n",
    "    ref_sentences = []\n",
    "    prompt_lengths = [2, 3, 4, 5, 6]\n",
    "    \n",
    "    # Strip data of non-text\n",
    "    for batch in dataloader:\n",
    "        for batch_entry in batch:\n",
    "            ref_sentences.append(tokenizer.decode(batch_entry)\n",
    "                                 .replace(\"<|endoftext|>\", \"\")\n",
    "                                 .split())\n",
    "    \n",
    "    for prompt_length in prompt_lengths:\n",
    "        ref_sentences_without_prompt = []\n",
    "        texts_without_prompt = []      \n",
    "\n",
    "        for ref_sentence in tqdm(ref_sentences):\n",
    "            ref_sentence_wo_prompt = ref_sentence[prompt_length:]\n",
    "            if (len(ref_sentence_wo_prompt) > 3): # Because BLEU uses 4-gram, prediction must be long enough\n",
    "                prompt = \" \".join(ref_sentence[:prompt_length])\n",
    "\n",
    "                ref_sentence_length = len(ref_sentence)\n",
    "\n",
    "                text = generate(model.to('cpu'), tokenizer, prompt, entry_length=ref_sentence_length)\n",
    "                text = text.replace(\"<|endoftext|>\", \"\")\n",
    "\n",
    "                text_wo_prompt = text.split()[prompt_length:]\n",
    "\n",
    "                if (len(text_wo_prompt) > 3): # Because BLEU uses 4-gram\n",
    "                    texts_without_prompt.append(\" \".join(text_wo_prompt))\n",
    "                    #print(\" \".join(text_wo_prompt))\n",
    "                    ref_sentences_without_prompt.append(\" \".join(ref_sentence_wo_prompt))\n",
    "                    #print(\" \".join(ref_sentence_wo_prompt))\n",
    "\n",
    "        score = bleu(texts_without_prompt, ref_sentences_without_prompt)\n",
    "        print(score)    \n",
    "\n",
    "                \n",
    "def bleu(generated, target):\n",
    "    ref_bleu = []\n",
    "    gen_bleu = []\n",
    "    for l in generated:\n",
    "        gen_bleu.append(l.split())\n",
    "    for i,l in enumerate(target):\n",
    "        ref_bleu.append([l.split()])\n",
    "    score_bleu = corpus_bleu(ref_bleu, gen_bleu)\n",
    "    return score_bleu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"All i want is\"\n",
    "entry_length = 20\n",
    "\n",
    "# Load trained model\n",
    "file = open(\"large_model\", 'rb')\n",
    "best_model_state = pickle.load(file)\n",
    "file.close()\n",
    "\n",
    "best_model = GPT2LMHeadModel.from_pretrained('gpt2')\n",
    "best_model.load_state_dict(best_model_state)\n",
    "\n",
    "#bleu_score(best_model, test_dataloader)\n",
    "\n",
    "# --------- Trained model ---------\n",
    "generated_text = generate(best_model.to('cpu'), tokenizer, prompt, entry_length=entry_length)\n",
    "print(\"Finetuned model:\")\n",
    "print(generated_text)\n",
    "\n",
    "# --------- Non-trained model ---------\n",
    "model_plain = GPT2LMHeadModel.from_pretrained('gpt2')\n",
    "\n",
    "generated_text = generate(model_plain.to('cpu'), tokenizer, prompt, entry_length=entry_length)\n",
    "print(\"Not finetuned model:\")\n",
    "print(generated_text)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "vscode": {
   "interpreter": {
    "hash": "05c4c729e88282d67eca00fe45a6137539b4020d6694da1fe52091cd432aba7f"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
